# Gradient-Descent
## Intro
The aim of supervised machine learning algorithms is to best estimate a target function that maps input data onto output variables. In other words, input data is used to produce predictions in the output based on some function. The idea of gradient descent involves minimising the error associated with these predicted and expected outputs, known as the cost function. Therefore, to have an accurate model, it is imperative to have this cost function minimised so that the error value is as low as possible. For a given function, this means finding the minimum value. 
## Plain Vanilla Gradient Descent
Given a function and a starting point somewhere along this function, plain vanilla gradient descent essentially finds the path to the minimum point by taking steps and calculating the slope at each step taken. By doing so, not only can the direction of descent be determined but also the sizes of steps taken, since the gradient becomes much smaller the closer it gets the minimum point. The algorithm stops when the calculated gradient approaches zero or after reaching a user specified limit for number of steps taken. 
## Variations
Certain modifications can be made to optimise the plain vanilla gradient descent algorithm. Two such examples include Momentum and Nesterov Accelerated Gradient (NAG). Momentum considers the exponential moving average of past gradients as well as the current gradient being calculated at each iteration. This method has the potential to avoid getting stuck at saddle points since the momentum should carry you over the point. However, it is also susceptible to overshooting the minimum as a result. NAG builds on this further by using projected gradients in its calculations, which involves essentially ‘looking ahead’ to determine whether the gradient is flattening or reversing in direction. This prevents potential oscillations from occurring when computing the gradient path. 

![alt text](https://github.com/booki96/Gradient-Descent/blob/master/vanilla.png)
